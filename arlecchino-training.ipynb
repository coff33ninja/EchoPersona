{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11564215,"sourceType":"datasetVersion","datasetId":7250751}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/djkruger/arlecchino-training?scriptVersionId=237059969\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Verify Structure:\nimport os\narlecchino_voice_data_path = kagglehub.dataset_download('djkruger/arlecchino-voice-data')\n\ndataset_path = \"../input/djkruger/arlecchino-voice-data\"  # Adjust charactername\nwavs_path = os.path.join(dataset_path, \"/kaggle/input/arlecchino-voice-data/wavs\")\nmetadata_path = os.path.join(dataset_path, \"/kaggle/input/arlecchino-voice-data/metadata.csv\")\nprint(f\"WAVs found: {len(os.listdir(wavs_path))}\")\nprint(f\"Metadata exists: {os.path.exists(metadata_path)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:47:54.893254Z","iopub.execute_input":"2025-04-29T14:47:54.893819Z","iopub.status.idle":"2025-04-29T14:47:54.902697Z","shell.execute_reply.started":"2025-04-29T14:47:54.893795Z","shell.execute_reply":"2025-04-29T14:47:54.90179Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1567922540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Verify Structure:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marlecchino_voice_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'djkruger/arlecchino-voice-data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../input/djkruger/arlecchino-voice-data\"\u001b[0m  \u001b[0;31m# Adjust charactername\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'kagglehub' is not defined"],"ename":"NameError","evalue":"name 'kagglehub' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport subprocess\nimport time\nimport shutil\nimport csv\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport json\nimport traceback\nfrom multiprocessing import Process, Queue, Pool, cpu_count\nimport select\nimport pty\nimport random\nimport queue\nimport uuid\nimport torch\n\n# --- Initial User Reminders ---\nprint(\"=\"*60)\nprint(\"IMPORTANT KAGGLE SETTINGS:\")\nprint(\"- Ensure 'Accelerator' is set to GPU (P100 or T4 x2 recommended).\")\nprint(\"- Ensure 'Internet' is turned ON in the Settings panel.\")\nprint(\"=\"*60)\n\n# Step 1: Set Up Environment\ncached_env_dir = \"/kaggle/input/rvc-dependencies\"\nenv_yml = \"/kaggle/working/environment.yml\"\ncudnn_dir = \"/kaggle/working/cudnn\"\ncupti_dir = \"/kaggle/working/cupti\"\n\n# Enhanced disk space check\nprint(\"Checking disk space...\")\ndisk_space = subprocess.run([\"df\", \"-h\", \"/kaggle/working\"], capture_output=True, text=True)\nif \"Avail\" in disk_space.stdout:\n    avail_space_line = [line for line in disk_space.stdout.splitlines() if \"/kaggle/working\" in line]\n    if avail_space_line:\n        avail_space = avail_space_line[0]\n        try:\n            avail_str = avail_space.split()[3]\n            unit = avail_str[-1].upper()\n            value = float(avail_str[:-1])\n            if unit == 'T': value *= 1024\n            elif unit == 'M': value /= 1024\n            elif unit == 'K': value /= (1024 * 1024)\n            print(f\"Available space: {value:.2f} GB\")\n            if value < 5:\n                raise RuntimeError(f\"Insufficient disk space: {value:.2f}GB available, need ~5GB\")\n        except (IndexError, ValueError) as e:\n            print(f\"Warning: Could not parse available disk space: {e}. Proceeding cautiously.\")\n    else:\n        print(\"Warning: Could not find /kaggle/working in df output. Disk space check skipped.\")\n\n# Create environment.yml\nenv_content = \"\"\"\nname: rvc\nchannels:\n  - conda-forge\n  - pytorch\ndependencies:\n  - python=3.11\n  - pip\n  - pytorch=2.3.1\n  - torchaudio=2.3.1\n  - torchvision=0.18.1\n  - cudatoolkit=11.8\n  - librosa=0.9.2\n  - soundfile=0.12.1\n  - scipy=1.11.4\n  - faiss-cpu=1.7.4\n  - fairseq=0.12.2\n  - gradio=3.14.0\n  - gradio-client=0.0.2\n  - fastapi=0.112.2\n  - uvicorn=0.30.6\n  - pydantic=2.8.2\n  - starlette=0.38.2\n  - httpx=0.27.0\n  - websockets=12.0\n  - python-multipart=0.0.9\n  - ffmpeg-python=0.2.0\n  - python-dotenv=1.0.1\n  - praat-parselmouth=0.4.3\n  - numpy=1.23.5\n  - numba=0.57.0\n  - pyworld=0.3.4\n  - pytorch-lightning=2.0.9\n  - joblib=1.4.2\n  - tensorboard=2.15.2\n  - transformers=4.28.1\n  - wandb=0.15.12\n  - openai-whisper=20231117\n  - matplotlib=3.7.5\n  - scikit-learn=1.3.2\n  - resampy=0.4.3\n  - tqdm=4.66.5\n  - einops=0.8.0\n  - tensorboardX=2.6.2.2\n  - pyngrok=7.2.0\n  - pip:\n      - wheel\n\"\"\"\nwith open(env_yml, \"w\") as f:\n    f.write(env_content)\n\n# Install cuDNN\nprint(\"Installing cuDNN for CUDA 11.8...\")\ntry:\n    !apt-get update -qq\n    !apt-get install -y --allow-downgrades libcudnn8=8.9.7.29-1+cuda11.8 libcudnn8-dev=8.9.7.29-1+cuda11.8\n    !ldconfig\nexcept subprocess.CalledProcessError as e:\n    print(f\"apt-get install failed: {e.stderr}; downloading cuDNN manually...\")\n    os.makedirs(cudnn_dir, exist_ok=True)\n    cudnn_url = \"https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-8.9.7.29_cuda11-archive.tar.xz\"\n    cudnn_tar = f\"{cudnn_dir}/cudnn.tar.xz\"\n    !wget -q {cudnn_url} -O {cudnn_tar}\n    !tar -xJf {cudnn_tar} -C {cudnn_dir} --strip-components=1\n    !cp -P {cudnn_dir}/lib/* /usr/lib/x86_64-linux-gnu/\n    !cp {cudnn_dir}/include/* /usr/include/\n    !ldconfig\n    !ls -l /usr/lib/x86_64-linux-gnu/libcudnn.so.8*\n\n# Remove residual libcupti files\nprint(\"Checking for residual libcupti files...\")\npotential_paths = [\n    \"/lib/x86_64-linux-gnu/libcupti.so\", \"/usr/lib/x86_64-linux-gnu/libcupti.so\",\n    \"/lib/x86_64-linux-gnu/libcupti.so.11.8\", \"/usr/lib/x86_64-linux-gnu/libcupti.so.11.8\",\n    \"/lib/x86_64-linux-gnu/libcupti.so.12\", \"/usr/lib/x86_64-linux-gnu/libcupti.so.12\",\n]\nremoved_flag = False\nfor path in potential_paths:\n    if os.path.exists(path) or os.path.islink(path):\n        !rm -f {path}\n        print(f\"Removed residual {path}\")\n        removed_flag = True\nif removed_flag:\n    !ldconfig\n\n# Install libcupti\nprint(\"Installing libcupti for CUDA 11.8...\")\nos.makedirs(cupti_dir, exist_ok=True)\ncupti_url = \"https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-cupti-11-8_11.8.87-1_amd64.deb\"\ncupti_deb = f\"{cupti_dir}/cuda-cupti-11-8.deb\"\ncupti_install_dir = f\"{cupti_dir}/cuda_install\"\ntry:\n    !wget -q {cupti_url} -O {cupti_deb}\n    !dpkg -x {cupti_deb} {cupti_install_dir}\n    cupti_so_path = f\"{cupti_install_dir}/usr/local/cuda-11.8/extras/CUPTI/lib64/libcupti.so.11.8\"\n    target_lib_path = \"/usr/lib/x86_64-linux-gnu/libcupti.so.11.8\"\n    target_link_path = \"/usr/lib/x86_64-linux-gnu/libcupti.so\"\n    if os.path.exists(cupti_so_path):\n        !cp {cupti_so_path} {target_lib_path}\n        !ln -sf {target_lib_path} {target_link_path}\n        !ldconfig\n        !ls -l {target_lib_path} {target_link_path}\n    else:\n        raise FileNotFoundError(f\"libcupti.so.11.8 not found at {cupti_so_path}\")\nexcept Exception as e:\n    print(f\"CUPTI .deb install failed: {e}. Attempting CUDA Toolkit...\")\n    toolkit_url = \"https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\"\n    toolkit_run = f\"{cupti_dir}/cuda_11.8.0.run\"\n    toolkit_install_path = f\"{cupti_dir}/cuda_toolkit_install\"\n    !wget -q {toolkit_url} -O {toolkit_run}\n    !chmod +x {toolkit_run}\n    !sh {toolkit_run} --silent --toolkit --toolkitpath={toolkit_install_path}\n    cupti_so_path = f\"{toolkit_install_path}/extras/CUPTI/lib64/libcupti.so.11.8\"\n    if os.path.exists(cupti_so_path):\n        !cp {cupti_so_path} {target_lib_path}\n        !ln -sf {target_lib_path} {target_link_path}\n        !ldconfig\n        !ls -l {target_lib_path} {target_link_path}\n    else:\n        shutil.rmtree(cupti_dir, ignore_errors=True)\n        raise RuntimeError(f\"Failed to find libcupti.so.11.8 at {cupti_so_path}\")\n\n# Set LD_LIBRARY_PATH\nstandard_paths = \"/usr/lib/x86_64-linux-gnu:/lib/x86_64-linux-gnu\"\ncurrent_ld_path = os.environ.get('LD_LIBRARY_PATH', '')\nos.environ[\"LD_LIBRARY_PATH\"] = f\"{standard_paths}:{current_ld_path}\" if current_ld_path else standard_paths\nprint(f\"LD_LIBRARY_PATH: {os.environ['LD_LIBRARY_PATH']}\")\n\n# Verify installations\nprint(\"\\n--- Verifying Installations ---\")\n!nvcc --version\n!nvidia-smi\n!find /usr/lib /lib -name 'libcudnn.so.8*' -ls 2>/dev/null || echo \"libcudnn.so.8 not found\"\n!find /usr/lib /lib -name 'libcupti.so.11.8*' -ls 2>/dev/null || echo \"libcupti.so.11.8 not found\"\n!ldconfig -p | grep libcupti || echo \"libcupti not found in linker cache\"\nif os.path.exists(\"/usr/lib/x86_64-linux-gnu/libcupti.so.11.8\"):\n    !readelf -V /usr/lib/x86_64-linux-gnu/libcupti.so.11.8 | grep -i version || echo \"Could not read version info\"\n\n# Test torch import\nprint(\"\\n--- Testing torch import and CUDA ---\")\ntry:\n    import torch\n    print(f\"Torch version: {torch.__version__}\")\n    cuda_available = torch.cuda.is_available()\n    print(f\"Torch CUDA available: {cuda_available}\")\n    if cuda_available:\n        print(f\"CUDA version: {torch.version.cuda}\")\n        print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n        num_gpus = torch.cuda.device_count()\n        print(f\"GPUs: {num_gpus}\")\n        for i in range(num_gpus):\n            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n    else:\n        raise RuntimeError(\"CUDA not available. Ensure GPU is enabled.\")\nexcept ImportError as e:\n    print(f\"FATAL: Torch import failed: {e}\")\n    !ldconfig -p | grep libcupti\n    !find /lib /usr -name 'libcupti.so*' -ls\n    raise\n\n# Detect GPU configuration\ngpu_count = torch.cuda.device_count()\ngpu_names = [torch.cuda.get_device_name(i) for i in range(gpu_count)]\nis_p100 = any(\"Tesla P100\" in name for name in gpu_names)\nis_t4_x2 = all(\"Tesla T4\" in name for name in gpu_names) and gpu_count == 2\nprint(f\"\\n--- GPU Configuration ---\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU count: {gpu_count}\")\nfor i, name in enumerate(gpu_names):\n    print(f\"GPU {i}: {name}, VRAM: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\nprint(\"Detected:\", \"Tesla P100\" if is_p100 else \"T4 x2\" if is_t4_x2 else f\"Unknown ({gpu_names})\")\n!nvidia-smi\n\n# Install dependencies\nif os.path.exists(cached_env_dir) and os.listdir(cached_env_dir):\n    print(f\"Using cached dependencies from {cached_env_dir}\")\n    !cp -rT {cached_env_dir} /usr/local/lib/python3.11/dist-packages/\n    !ldconfig\nelse:\n    print(\"Installing dependencies with Mamba...\")\n    !command -v mamba >/dev/null 2>&1 || conda install -y mamba -n base -c conda-forge\n    !mamba env update -n base -f {env_yml} --prune\n    !apt-get update -qq\n    !apt-get install -y --no-install-recommends \\\n        build-essential g++ \\\n        libopenblas-dev \\\n        unzip cmake \\\n        libvorbis-dev libmpg123-dev libsndfile1-dev \\\n        swig libomp-dev aria2 git\n    !apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Verify packages\nprint(\"\\n--- Verifying packages ---\")\npython_executable = \"/opt/conda/bin/python\"\n!{python_executable} -c \"import torch; print(f'Torch: {torch.__version__}')\"\n!{python_executable} -c \"import librosa; print(f'Librosa: {librosa.__version__}')\"\n!{python_executable} -c \"import soundfile; print(f'Soundfile: {soundfile.__version__}')\"\n!{python_executable} -c \"import fairseq; print(f'Fairseq: {fairseq.__version__}')\"\n!{python_executable} -c \"import gradio; print(f'Gradio: {gradio.__version__}')\"\n!{python_executable} -c \"import transformers; print(f'Transformers: {transformers.__version__}')\"\n!{python_executable} -c \"import whisper; print(f'Whisper: {whisper.__version__}')\"\n!aria2c --version | head -n 1\n!ffmpeg -version | head -n 1\n\n# Step 2: Install VGMStream\nprint(\"\\n--- Step 2: Install VGMStream ---\")\ntools_dir = \"/kaggle/working/tools\"\nvgmstream_dir = f\"{tools_dir}/vgmstream_cli_extracted\"\nos.makedirs(vgmstream_dir, exist_ok=True)\nvgmstream_zip = f\"{tools_dir}/vgmstream-linux-cli.zip\"\nvgmstream_url = \"https://github.com/vgmstream/vgmstream/releases/download/r1980/vgmstream-linux-cli.zip\"\nvgmstream_cli = f\"{vgmstream_dir}/vgmstream-cli\"\n\nif os.path.exists(vgmstream_cli):\n    print(f\"VGMStream CLI exists at {vgmstream_cli}, skipping download.\")\nelse:\n    if os.path.exists(vgmstream_zip) and os.path.getsize(vgmstream_zip) > 0:\n        print(f\"VGMStream ZIP exists: {vgmstream_zip}\")\n    else:\n        for attempt in range(3):\n            print(f\"Downloading VGMStream (attempt {attempt + 1}/3)...\")\n            try:\n                !wget --tries=2 --timeout=30 -q -O {vgmstream_zip} {vgmstream_url}\n                print(\"Downloaded VGMStream ZIP\")\n                break\n            except subprocess.CalledProcessError as e:\n                print(f\"Download failed: {e.stderr}\")\n                if attempt < 2:\n                    time.sleep(5)\n                else:\n                    vgmstream_zip = None\n\n    if vgmstream_zip and os.path.exists(vgmstream_zip):\n        print(f\"Unzipping {vgmstream_zip}...\")\n        if os.path.exists(vgmstream_dir):\n            shutil.rmtree(vgmstream_dir)\n        os.makedirs(vgmstream_dir)\n        !unzip -o {vgmstream_zip} -d {vgmstream_dir}\n        if os.path.exists(vgmstream_cli):\n            print(f\"Found vgmstream-cli at: {vgmstream_cli}\")\n            os.remove(vgmstream_zip)\n            print(f\"Deleted {vgmstream_zip}\")\n        else:\n            print(f\"Warning: vgmstream-cli not found in {vgmstream_dir}\")\n            vgmstream_cli = None\n\nif vgmstream_cli and os.path.exists(vgmstream_cli):\n    try:\n        !chmod +x {vgmstream_cli}\n        result = subprocess.run([vgmstream_cli, \"--version\"], capture_output=True, text=True, check=True)\n        print(f\"vgmstream-cli version: {result.stdout.strip()}\")\n    except Exception as e:\n        print(f\"VGMStream verification failed: {e}\")\n        vgmstream_cli = None\nelse:\n    print(\"Attempting to build VGMStream from source...\")\n    vgmstream_src_dir = f\"{tools_dir}/vgmstream_src\"\n    build_dir = f\"{vgmstream_src_dir}/build\"\n    final_cli_path = f\"{build_dir}/vgmstream-cli\"\n    try:\n        !apt-get update -qq\n        !apt-get install -y libvorbis-dev libmpg123-dev libsndfile1-dev\n        if not os.path.exists(vgmstream_src_dir):\n            !git clone --depth 1 https://github.com/vgmstream/vgmstream.git {vgmstream_src_dir}\n        os.makedirs(build_dir, exist_ok=True)\n        !cmake .. -B {build_dir}\n        !make -C {build_dir} -j{cpu_count()}\n        if os.path.exists(final_cli_path):\n            vgmstream_cli = final_cli_path\n            !chmod +x {vgmstream_cli}\n            result = subprocess.run([vgmstream_cli, \"--version\"], capture_output=True, text=True, check=True)\n            print(f\"VGMStream built: {result.stdout.strip()}\")\n        else:\n            raise FileNotFoundError(f\"Build failed: {final_cli_path} not found\")\n    except Exception as e:\n        print(f\"Failed to build VGMStream: {e}\")\n        vgmstream_cli = None\n\n# Fallback to ww2ogg\nww2ogg_available = False\nif not vgmstream_cli:\n    print(\"Trying ww2ogg as fallback...\")\n    try:\n        !pip install --upgrade pip\n        !pip install ww2ogg==0.9.0\n        import ww2ogg\n        print(\"ww2ogg installed\")\n        ww2ogg_available = True\n    except Exception as e:\n        print(f\"Failed to install ww2ogg: {e}\")\nelse:\n    print(\"Using VGMStream for WEM conversion\")\n\n# Step 3: Install and Configure Ngrok\nprint(\"\\n--- Step 3: Install Ngrok ---\")\ntry:\n    !pip install --upgrade pyngrok==7.2.0\n    from pyngrok import ngrok\n    print(\"pyngrok installed\")\n\n    NGROK_TOKEN = None\n    try:\n        from kaggle_secrets import UserSecretsClient\n        NGROK_TOKEN = UserSecretsClient().get_secret(\"NGROK_AUTH_TOKEN\")\n        print(\"Retrieved Ngrok token from Kaggle Secrets\")\n    except Exception as e:\n        print(f\"Could not retrieve Ngrok token: {e}\")\n        print(\"Set NGROK_TOKEN manually or add it as Kaggle Secret 'NGROK_AUTH_TOKEN'\")\n\n    if not NGROK_TOKEN:\n        print(\"Ngrok token not set. Falling back to Pinggy.\")\n    else:\n        try:\n            ngrok.set_auth_token(NGROK_TOKEN)\n            print(\"Ngrok authtoken configured\")\n        except Exception as e:\n            print(f\"Ngrok configuration failed: {e}. Falling back to Pinggy.\")\n            NGROK_TOKEN = None\nexcept Exception as e:\n    print(f\"Failed to install pyngrok: {e}\")\n    NGROK_TOKEN = None\n\n# Step 4: Clone RVC Repositories and Download Models\nprint(\"\\n--- Step 4: Clone RVC Repositories ---\")\nos.chdir(\"/kaggle/working\")\nprint(f\"Current directory: {os.getcwd()}\")\n\ndef check_connectivity(url, service_name):\n    print(f\"Testing {service_name} connectivity ({url})...\")\n    try:\n        result = subprocess.run([\"curl\", \"-I\", \"--connect-timeout\", \"10\", url], capture_output=True, text=True, check=True)\n        if \"HTTP/2 200\" in result.stdout or \"HTTP/1.1 200\" in result.stdout:\n            print(f\"{service_name} connectivity verified\")\n            return True\n        print(f\"Warning: Unexpected response:\\n{result.stdout}\")\n        return False\n    except Exception as e:\n        print(f\"Warning: {service_name} connectivity test failed: {e}\")\n        return False\n\ncheck_connectivity(\"https://github.com\", \"GitHub\")\ncheck_connectivity(\"https://huggingface.co\", \"Hugging Face\")\n\nrvc_repo_url = \"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI.git\"\nrvc_dir = \"/kaggle/working/RVC\"\nwebui_repo_url = \"https://github.com/ddPn08/rvc-webui.git\"\nwebui_dir = \"/kaggle/working/rvc-webui\"\n\ndef clone_repo(repo_url, target_dir, max_retries=3):\n    print(f\"Cloning {repo_url} into {target_dir}...\")\n    if os.path.exists(target_dir):\n        shutil.rmtree(target_dir)\n    for attempt in range(max_retries):\n        print(f\"Clone attempt {attempt + 1}/{max_retries}...\")\n        try:\n            result = subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", repo_url, target_dir], check=True, capture_output=True, text=True, timeout=120)\n            print(f\"Cloned {os.path.basename(target_dir)}\")\n            if target_dir == rvc_dir and not os.path.exists(os.path.join(rvc_dir, \"infer\", \"modules\", \"train\")):\n                raise FileNotFoundError(\"Key directory 'infer/modules/train' missing\")\n            if target_dir == webui_dir and not any(os.path.exists(os.path.join(webui_dir, f)) for f in [\"launch.py\", \"app.py\", \"webui.py\", \"main.py\"]):\n                raise FileNotFoundError(\"No entry point found\")\n            return True\n        except Exception as e:\n            print(f\"Clone failed: {e}\")\n            if os.path.exists(target_dir):\n                shutil.rmtree(target_dir, ignore_errors=True)\n            if attempt < max_retries - 1:\n                time.sleep(5 * (2 ** attempt))\n            else:\n                return False\n\nif not clone_repo(rvc_repo_url, rvc_dir):\n    raise RuntimeError(f\"Failed to clone RVC repository\")\nif not clone_repo(webui_repo_url, webui_dir):\n    raise RuntimeError(f\"Failed to clone WebUI fork\")\n\n# Download pretrained models\nprint(\"\\n--- Downloading Pretrained Models ---\")\npretrained_v2_dir = os.path.join(rvc_dir, \"pretrained_v2\")\nuvr5_weights_dir = os.path.join(rvc_dir, \"uvr5_weights\")\nweights_dir = os.path.join(rvc_dir, \"weights\")\nhubert_path = os.path.join(rvc_dir, \"hubert_base.pt\")\nos.makedirs(pretrained_v2_dir, exist_ok=True)\nos.makedirs(uvr5_weights_dir, exist_ok=True)\nos.makedirs(weights_dir, exist_ok=True)\n\nmodel_urls = [\n    (\"https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0G40k.pth\", os.path.join(pretrained_v2_dir, \"f0G40k.pth\"), 150),\n    (\"https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0D40k.pth\", os.path.join(pretrained_v2_dir, \"f0D40k.pth\"), 150),\n    (\"https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/uvr5_weights/HP5-主旋律人声vocals+其他instrumentals.pth\", os.path.join(uvr5_weights_dir, \"HP5-主旋律人声vocals+其他instrumentals.pth\"), 150),\n    (\"https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt\", hubert_path, 350),\n    (\"https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/rmvpe.pt\", os.path.join(weights_dir, \"rmvpe.pt\"), 5)\n]\n\ndef download_file_robust(url, output_path, estimated_size_mb, max_aria_retries=2, max_wget_retries=2):\n    filename = os.path.basename(output_path)\n    output_dir = os.path.dirname(output_path)\n    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n        print(f\"Model exists: {filename}, skipping download\")\n        return True\n    print(f\"Checking disk space for {filename} (~{estimated_size_mb}MB)...\")\n    try:\n        stat = os.statvfs('/kaggle/working')\n        available_mb = (stat.f_bavail * stat.f_frsize) / (1024 * 1024)\n        if available_mb < estimated_size_mb * 1.1:\n            print(f\"Error: Insufficient disk space: {available_mb:.1f}MB available, need ~{estimated_size_mb * 1.1:.1f}MB\")\n            return False\n    except Exception as e:\n        print(f\"Warning: Could not check disk space: {e}. Proceeding cautiously.\")\n\n    aria2c_available = shutil.which(\"aria2c\") is not None\n    if aria2c_available:\n        for attempt in range(max_aria_retries):\n            print(f\"Downloading {filename} with aria2c (attempt {attempt + 1}/{max_aria_retries})...\")\n            try:\n                cmd = [\"aria2c\", \"--console-log-level=warn\", \"-c\", \"-x\", \"4\", \"-s\", \"4\", \"-k\", \"1M\", \"--connect-timeout=15\", \"--timeout=60\", \"--dir\", output_dir, \"-o\", filename, url]\n                result = subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=300)  # Increased timeout\n                print(f\"aria2c successful for {filename}\")\n                if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n                    return True\n                print(f\"aria2c ran but {filename} is missing or empty\")\n                if os.path.exists(output_path):\n                    os.remove(output_path)\n            except subprocess.TimeoutExpired:\n                print(f\"aria2c timed out after 300s\")\n                if os.path.exists(output_path):\n                    os.remove(output_path)\n            except KeyboardInterrupt:\n                print(f\"KeyboardInterrupt during aria2c for {filename}. Cleaning up...\")\n                if os.path.exists(output_path):\n                    os.remove(output_path)\n                return False\n            except Exception as e:\n                print(f\"aria2c failed: {e}\")\n                if os.path.exists(output_path):\n                    os.remove(output_path)\n            if attempt < max_aria_retries - 1:\n                print(\"Retrying after delay...\")\n                time.sleep(10 * (2 ** attempt))  # Increased delay\n    \n    print(f\"Falling back to wget for {filename}...\")\n    wget_available = shutil.which(\"wget\") is not None\n    if not wget_available:\n        print(\"Error: wget not found\")\n        return False\n    for attempt in range(max_wget_retries):\n        print(f\"Downloading {filename} with wget (attempt {attempt + 1}/{max_wget_retries})...\")\n        try:\n            cmd = [\"wget\", \"--continue\", \"--tries=2\", \"--timeout=60\", \"-O\", output_path, url]\n            result = subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=300)  # Increased timeout\n            print(f\"wget successful for {filename}\")\n            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n                return True\n            print(f\"wget ran but {filename} is missing or empty\")\n            if os.path.exists(output_path):\n                os.remove(output_path)\n        except subprocess.TimeoutExpired:\n            print(f\"wget timed out after 300s\")\n            if os.path.exists(output_path):\n                os.remove(output_path)\n        except KeyboardInterrupt:\n            print(f\"KeyboardInterrupt during wget for {filename}. Cleaning up...\")\n            if os.path.exists(output_path):\n                os.remove(output_path)\n            return False\n        except Exception as e:\n            print(f\"wget failed: {e}\")\n            if os.path.exists(output_path):\n                os.remove(output_path)\n        if attempt < max_wget_retries - 1:\n            print(\"Retrying after delay...\")\n            time.sleep(10 * (2 ** attempt))  # Increased delay\n    print(f\"Failed to download {filename}\")\n    return False\n\nall_models_downloaded = True\nfor url, path, size_mb in model_urls:\n    if not download_file_robust(url, path, size_mb):\n        all_models_downloaded = False\n        print(f\"CRITICAL: Failed to download {os.path.basename(path)}\")\nif not all_models_downloaded:\n    missing_models = [path for _, path, _ in model_urls if not (os.path.exists(path) and os.path.getsize(path) > 0)]\n    raise FileNotFoundError(f\"Missing models: {missing_models}\")\nprint(\"All pretrained models downloaded or present\")\n\nprint(\"\\nVerifying models:\")\n!ls -lh {rvc_dir}/pretrained_v2/*.pth || echo \"No pth files in pretrained_v2\"\n!ls -lh {rvc_dir}/uvr5_weights/*.pth || echo \"No pth files in uvr5_weights\"\n!ls -lh {rvc_dir}/hubert_base.pt || echo \"hubert_base.pt not found\"\n!ls -lh {rvc_dir}/weights/rmvpe.pt || echo \"rmvpe.pt not found\"\n\n# Step 5: Set Up Dataset\nprint(\"\\n--- Step 5: Set Up Dataset ---\")\ndataset_root = \"/kaggle/input/arlecchino-voice-data\"\noutput_path = \"/kaggle/working/processed_dataset\"\ntemp_wav_path = \"/kaggle/working/temp_wavs\"\noutput_wav_dir = os.path.join(output_path, \"wavs\")\nmetadata_dst = os.path.join(output_path, \"metadata.csv\")\nmetadata_train_dst = os.path.join(output_path, \"metadata_train.csv\")\nmetadata_val_dst = os.path.join(output_path, \"metadata_val.csv\")\nTARGET_SR = 44100\nSPEAKER_NAME = \"Arlecchino\"\nMIN_FILES_FOR_CACHE = 500\nVALIDATION_SPLIT_RATIO = 0.05\nMIN_VAL_SAMPLES = 5\n\nuse_cached_dataset = False\nif os.path.exists(output_wav_dir) and os.path.exists(metadata_train_dst) and os.path.exists(metadata_val_dst):\n    try:\n        wav_count = len([f for f in os.listdir(output_wav_dir) if f.lower().endswith(\".wav\")])\n        train_count = sum(1 for _ in open(metadata_train_dst, \"r\", encoding='utf-8') if _.strip())\n        val_count = sum(1 for _ in open(metadata_val_dst, \"r\", encoding='utf-8') if _.strip())\n        print(f\"Cached dataset: {wav_count} WAVs, {train_count} train, {val_count} val\")\n        if wav_count >= MIN_FILES_FOR_CACHE and (train_count + val_count) >= MIN_FILES_FOR_CACHE * 0.95:\n            print(\"Using cached dataset\")\n            use_cached_dataset = True\n            !ls -l {output_wav_dir} | head -n 5\n            !echo \"Total files:\" $(ls -1 {output_wav_dir} | wc -l)\n            !echo \"Train lines:\" $(wc -l < {metadata_train_dst})\n            !echo \"Val lines:\" $(wc -l < {metadata_val_dst})\n            !echo \"Train head:\"; !head -n 3 {metadata_train_dst}\n            !echo \"Val head:\"; !head -n 3 {metadata_val_dst}\n        else:\n            print(\"Cached dataset incomplete. Reprocessing...\")\n            shutil.rmtree(output_path, ignore_errors=True)\n    except Exception as e:\n        print(f\"Error checking cache: {e}. Reprocessing...\")\n        shutil.rmtree(output_path, ignore_errors=True)\nelse:\n    print(\"No complete cached dataset. Processing...\")\n\ndef convert_wem_file(args):\n    input_file, output_file, tool_path_or_flag = args\n    filename = os.path.basename(input_file)\n    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n        return (input_file, None)\n    try:\n        if tool_path_or_flag == \"ww2ogg\":\n            from ww2ogg import convert\n            convert.convert_file(input_file, output_file)\n        elif tool_path_or_flag and os.path.exists(tool_path_or_flag):\n            cmd = [tool_path_or_flag, \"-o\", output_file, input_file]\n            subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=60)\n        else:\n            return (input_file, \"No valid conversion tool\")\n        if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:\n            return (input_file, \"Conversion ran but output is missing or empty\")\n        return (input_file, None)\n    except Exception as e:\n        return (input_file, f\"Conversion failed: {e}\")\n\ndef convert_wem_to_wav_parallel(input_dir, output_dir, conversion_tool):\n    os.makedirs(output_dir, exist_ok=True)\n    input_files_args = []\n    import glob\n    wem_patterns = [os.path.join(input_dir, \"*.wem\"), os.path.join(input_dir, \"*.wem.wav\")]\n    found_files = []\n    for pattern in wem_patterns:\n        found_files.extend(glob.glob(pattern))\n    if not found_files:\n        print(f\"Warning: No WEM files in {input_dir}\")\n        return []\n    for input_path in found_files:\n        f = os.path.basename(input_path)\n        base, _ = os.path.splitext(f)\n        if base.lower().endswith(\".wem\"):\n            base, _ = os.path.splitext(base)\n        output_filename = f\"{base}.wav\"\n        output_path = os.path.join(output_dir, output_filename)\n        input_files_args.append((input_path, output_path, conversion_tool))\n    if not input_files_args:\n        print(f\"Warning: No WEM files identified\")\n        return []\n    print(f\"Converting {len(input_files_args)} WEM files with {cpu_count()} processes...\")\n    failed_files = []\n    successful_count = 0\n    with Pool(processes=cpu_count()) as pool:\n        results = pool.map(convert_wem_file, input_files_args)\n    for input_f, error in results:\n        if error:\n            failed_files.append((os.path.basename(input_f), error))\n        else:\n            successful_count += 1\n    print(f\"Conversion complete. Successful: {successful_count}, Failed: {len(failed_files)}\")\n    if failed_files:\n        print(\"--- Conversion Failures ---\")\n        for fname, err in failed_files[:10]:\n            print(f\"  File: {fname}, Error: {err}\")\n        if len(failed_files) > 10:\n            print(f\"  ... and {len(failed_files) - 10} more\")\n    return failed_files\n\ndef preprocess_audio_file(args):\n    input_file, output_file, target_sr = args\n    filename = os.path.basename(input_file)\n    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n        return (input_file, None)\n    try:\n        y, sr = librosa.load(input_file, sr=target_sr, mono=True)\n        if len(y) == 0:\n            return (input_file, \"Empty audio\")\n        max_amp = np.max(np.abs(y))\n        if max_amp > 0:\n            y = y / max_amp * 0.95\n        else:\n            y = np.zeros_like(y)\n        sf.write(output_file, y, target_sr, subtype='PCM_16')\n        return (input_file, None)\n    except Exception as e:\n        return (input_file, f\"Preprocessing error: {e}\")\n\ndef preprocess_audio_parallel(input_dir, output_dir, target_sr):\n    os.makedirs(output_dir, exist_ok=True)\n    files_to_process = []\n    import glob\n    wav_pattern = os.path.join(input_dir, \"*.wav\")\n    found_wavs = glob.glob(wav_pattern)\n    if not found_wavs:\n        print(f\"Warning: No WAV files in {wav_pattern}\")\n        return []\n    for input_path in found_wavs:\n        f = os.path.basename(input_path)\n        output_path = os.path.join(output_dir, f)\n        files_to_process.append((input_path, output_path, target_sr))\n    print(f\"Preprocessing {len(files_to_process)} WAV files with {cpu_count()} processes...\")\n    failed_files = []\n    successful_count = 0\n    with Pool(processes=cpu_count()) as pool:\n        results = pool.map(preprocess_audio_file, files_to_process)\n    for input_f, error in results:\n        if error:\n            failed_files.append((os.path.basename(input_f), error))\n        else:\n            successful_count += 1\n    print(f\"Preprocessing complete. Successful: {successful_count}, Failed: {len(failed_files)}\")\n    if failed_files:\n        print(\"--- Preprocessing Failures ---\")\n        for fname, err in failed_files[:10]:\n            print(f\"  File: {fname}, Error: {err}\")\n        if len(failed_files) > 10:\n            print(f\"  ... and {len(failed_files) - 10} more\")\n    return failed_files\n\ndef rename_potential_duplicates(dataset_dir):\n    if shutil.which(\"rename\") is None:\n        print(\"Warning: 'rename' utility not found\")\n        return\n    print(\"Checking for duplicate files...\")\n    try:\n        cmd = [\"rename\", \"-v\", \"s/(\\\\.\\\\w+)~(\\\\d*)$/_$2$1/\", f\"{dataset_dir}/*.*~*\"]\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode == 0 and result.stdout.strip():\n            print(\"Renamed duplicates:\\n\", result.stdout)\n        else:\n            print(\"No duplicates found or renamed\")\n    except Exception as e:\n        print(f\"Error during rename: {e}\")\n\nif not use_cached_dataset:\n    print(\"\\n--- Starting Dataset Processing ---\")\n    os.makedirs(output_path, exist_ok=True)\n    os.makedirs(temp_wav_path, exist_ok=True)\n    os.makedirs(output_wav_dir, exist_ok=True)\n    input_wem_dir = os.path.join(dataset_root, \"wavs\")\n    if not os.path.isdir(input_wem_dir):\n        input_wem_dir = dataset_root\n        if not os.path.isdir(input_wem_dir):\n            raise FileNotFoundError(f\"Input audio directory not found at {input_wem_dir}\")\n        print(f\"Using dataset root {dataset_root} as input\")\n    conversion_tool_param = \"ww2ogg\" if ww2ogg_available else vgmstream_cli if vgmstream_cli else None\n    if conversion_tool_param:\n        print(f\"Using {conversion_tool_param} for conversion\")\n        conversion_failures = convert_wem_to_wav_parallel(input_wem_dir, temp_wav_path, conversion_tool_param)\n        if not any(f.lower().endswith(\".wav\") for f in os.listdir(temp_wav_path)):\n            print(f\"Warning: No WAV files in {temp_wav_path}\")\n    else:\n        print(\"Warning: No WEM conversion tool. Assuming WAV input\")\n        temp_wav_path = input_wem_dir\n    preprocessing_failures = preprocess_audio_parallel(temp_wav_path, output_wav_dir, TARGET_SR)\n    rename_potential_duplicates(output_wav_dir)\n    metadata_src = os.path.join(dataset_root, \"metadata.csv\")\n    print(f\"\\nLooking for metadata at: {metadata_src}\")\n    if os.path.exists(metadata_src):\n        print(\"Metadata found. Copying...\")\n        try:\n            shutil.copy(metadata_src, metadata_dst)\n            print(f\"Copied to {metadata_dst}\")\n        except Exception as e:\n            print(f\"Error copying metadata: {e}. Attempting transcription\")\n            metadata_src = None\n    else:\n        print(\"Metadata not found\")\n        metadata_src = None\n    if not os.path.exists(metadata_dst) or os.path.getsize(metadata_dst) == 0:\n        print(\"Generating metadata with Whisper...\")\n        try:\n            import whisper\n            print(\"Loading Whisper model...\")\n            if torch.cuda.is_available():\n                vram_free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n                device = \"cuda\" if vram_free > 2 * 1024**3 else \"cpu\"\n                print(f\"Using {device} for Whisper (VRAM free: {vram_free/1024**3:.2f}GB)\")\n            else:\n                device = \"cpu\"\n            model = whisper.load_model(\"base\", device=device)\n            print(\"Whisper loaded\")\n            transcription_errors = []\n            generated_count = 0\n            with open(metadata_dst, \"w\", encoding='utf-8', newline='') as f:\n                writer = csv.writer(f, delimiter='|', quoting=csv.QUOTE_MINIMAL)\n                wav_files = sorted([f for f in os.listdir(output_wav_dir) if f.lower().endswith(\".wav\")])\n                if not wav_files:\n                    print(\"Warning: No WAV files to transcribe\")\n                else:\n                    from tqdm import tqdm\n                    for file in tqdm(wav_files, desc=\"Transcribing\"):\n                        file_path = os.path.join(output_wav_dir, file)\n                        try:\n                            result = model.transcribe(file_path, fp16=(device == \"cuda\"))\n                            file_id = os.path.splitext(file)[0]\n                            transcription = result[\"text\"].strip().replace(\"\\n\", \" \")\n                            if not transcription:\n                                transcription_errors.append((file, \"Empty transcription\"))\n                                continue\n                            writer.writerow([file_id, SPEAKER_NAME, transcription])\n                            generated_count += 1\n                        except Exception as e:\n                            transcription_errors.append((file, str(e)))\n            print(f\"Generated {generated_count} metadata entries\")\n            if transcription_errors:\n                print(f\"{len(transcription_errors)} transcription errors\")\n                for fname, err in transcription_errors[:5]:\n                    print(f\"  File: {fname}, Error: {err}\")\n            del model\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except ImportError:\n            raise RuntimeError(\"Whisper package not found\")\n        except Exception as e:\n            raise RuntimeError(f\"Whisper transcription failed: {e}\")\n    if os.path.exists(metadata_dst) and os.path.getsize(metadata_dst) > 0:\n        print(f\"\\nSplitting {metadata_dst} into Train/Val...\")\n        try:\n            with open(metadata_dst, 'r', encoding='utf-8') as f:\n                all_lines = [line for line in f if line.strip()]\n            num_total = len(all_lines)\n            num_val = int(num_total * VALIDATION_SPLIT_RATIO)\n            num_train = num_total - num_val\n            if num_total < MIN_VAL_SAMPLES:\n                print(\"Dataset too small for validation. Using all for training\")\n                shutil.copy(metadata_dst, metadata_train_dst)\n                with open(metadata_val_dst, 'w') as f_val:\n                    pass\n                print(f\"Train: {num_total}, Val: 0\")\n            elif num_val < MIN_VAL_SAMPLES and num_total >= MIN_VAL_SAMPLES:\n                num_val = MIN_VAL_SAMPLES\n                num_train = num_total - num_val\n                print(f\"Adjusted split: Train={num_train}, Val={num_val}\")\n            else:\n                print(f\"Standard split: Train={num_train}, Val={num_val}\")\n            random.shuffle(all_lines)\n            train_lines = all_lines[:num_train]\n            val_lines = all_lines[num_train:]\n            with open(metadata_train_dst, 'w', encoding='utf-8') as f_train:\n                f_train.writelines(train_lines)\n            with open(metadata_val_dst, 'w', encoding='utf-8') as f_val:\n                f_val.writelines(val_lines)\n            print(f\"Train saved: {metadata_train_dst}\")\n            print(f\"Val saved: {metadata_val_dst}\")\n        except Exception as e:\n            print(f\"Error splitting metadata: {e}. Using combined file\")\n            if os.path.exists(metadata_dst):\n                shutil.copy(metadata_dst, metadata_train_dst)\n                shutil.copy(metadata_dst, metadata_val_dst)\n    else:\n        print(\"Error: Metadata file missing or empty\")\n        open(metadata_train_dst, 'w').close()\n        open(metadata_val_dst, 'w').close()\n    if temp_wav_path != input_wem_dir and os.path.exists(temp_wav_path):\n        print(f\"Cleaning up: {temp_wav_path}\")\n        shutil.rmtree(temp_wav_path, ignore_errors=True)\n\nprint(\"\\n--- Verifying Dataset ---\")\nfinal_wav_files = []\nif os.path.isdir(output_wav_dir):\n    final_wav_files = [f for f in os.listdir(output_wav_dir) if f.lower().endswith(\".wav\")]\n    print(f\"WAV files: {len(final_wav_files)}\")\n    !ls -l {output_wav_dir} | head -n 5\nelse:\n    print(f\"Error: {output_wav_dir} does not exist\")\ntrain_lines_count = 0\nval_lines_count = 0\nif os.path.exists(metadata_train_dst):\n    train_lines_count = sum(1 for line in open(metadata_train_dst, \"r\", encoding='utf-8') if line.strip())\n    print(f\"Train metadata lines: {train_lines_count}\")\n    !echo \"Train head:\"; !head -n 3 {metadata_train_dst}\nelse:\n    print(f\"Error: {metadata_train_dst} missing\")\nif os.path.exists(metadata_val_dst):\n    val_lines_count = sum(1 for line in open(metadata_val_dst, \"r\", encoding='utf-8') if line.strip())\n    print(f\"Val metadata lines: {val_lines_count}\")\n    !echo \"Val head:\"; !head -n 3 {metadata_val_dst}\nelse:\n    print(f\"Error: {metadata_val_dst} missing\")\nif not final_wav_files or train_lines_count == 0:\n    raise RuntimeError(\"Dataset processing failed: No WAV files or training metadata\")\n\n# Step 6: Create RVC Training Configuration\nprint(\"\\n--- Creating RVC Training Config ---\")\nconfig_content = {\n    \"train\": {\n        \"log_interval\": 100, \"seed\": 1234, \"epochs\": 200, \"learning_rate\": 1e-4,\n        \"betas\": [0.8, 0.99], \"eps\": 1e-9, \"batch_size\": 8,\n        \"fp16_run\": torch.cuda.is_available(), \"lr_decay\": 0.999875,\n        \"segment_size\": 12800, \"init_lr_ratio\": 1, \"warmup_epochs\": 0,\n        \"c_mel\": 45, \"c_kl\": 1.0\n    },\n    \"data\": {\n        \"training_files\": metadata_train_dst, \"validation_files\": metadata_val_dst,\n        \"max_wav_value\": 32768.0, \"sampling_rate\": TARGET_SR,\n        \"filter_length\": 2048, \"hop_length\": 240, \"win_length\": 1200,\n        \"n_mel_channels\": 80, \"mel_fmin\": 0.0, \"mel_fmax\": None\n    },\n    \"model\": {\n        \"inter_channels\": 192, \"hidden_channels\": 192, \"filter_channels\": 768,\n        \"n_heads\": 2, \"n_layers\": 6, \"kernel_size\": 3, \"p_dropout\": 0.1,\n        \"resblock\": \"1\", \"resblock_kernel_sizes\": [3, 7, 11],\n        \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n        \"upsample_rates\": [10, 6, 2, 2], \"upsample_initial_channel\": 512,\n        \"upsample_kernel_sizes\": [16, 16, 4, 4], \"n_layers_q\": 3,\n        \"use_spectral_norm\": False, \"gin_channels\": 256\n    },\n    \"experiment_name\": SPEAKER_NAME, \"save_every_epoch\": 10,\n    \"save_only_latest\": False, \"save_total_limit\": 0, \"f0method\": \"rmvpe\",\n    \"pretrained_G\": os.path.join(rvc_dir, \"pretrained_v2\", \"f0G40k.pth\"),\n    \"pretrained_D\": os.path.join(rvc_dir, \"pretrained_v2\", \"f0D40k.pth\"),\n    \"cache_data_in_ram\": True, \"num_workers\": min(4, cpu_count()),\n    \"pitch_guidance\": True, \"use_wandb\": False\n}\n\n# Set GPUs with fallback\ngpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0\nif gpu_count > 0:\n    config_content[\"gpus\"] = \",\".join(map(str, range(gpu_count)))\nelse:\n    config_content[\"gpus\"] = \"0\"  # Fallback to single GPU or dummy value\n    print(\"Warning: No GPUs detected. Setting 'gpus' to '0' as fallback.\")\n\n# Adjust batch size based on GPU\nif is_p100:\n    config_content[\"train\"][\"batch_size\"] = max(2, config_content[\"train\"][\"batch_size\"] // 3)\n    print(f\"Batch size for P100: {config_content['train']['batch_size']}\")\nelif is_t4_x2:\n    config_content[\"train\"][\"batch_size\"] = max(4, config_content[\"train\"][\"batch_size\"])\n    print(f\"Batch size for T4x2: {config_content['train']['batch_size']}\")\nelse:\n    config_content[\"train\"][\"batch_size\"] = max(4, config_content[\"train\"][\"batch_size\"] // 2)\n    print(f\"Batch size for Unknown GPU: {config_content['train']['batch_size']}\")\n\n# Save config\nconfig_dir = os.path.join(rvc_dir, \"configs\")\nos.makedirs(config_dir, exist_ok=True)\nconfig_path = os.path.join(config_dir, f\"{SPEAKER_NAME}_config.json\")\nwith open(config_path, \"w\", encoding='utf-8') as f:\n    json.dump(config_content, f, indent=4)\nprint(f\"Config saved: {config_path}\")\n\n# Verify config\nprint(\"Verifying config paths:\")\nprint(f\"  Train: {config_content['data']['training_files']} - {os.path.exists(config_content['data']['training_files'])}\")\nprint(f\"  Val: {config_content['data']['validation_files']} - {os.path.exists(config_content['data']['validation_files'])}\")\nprint(f\"  Pretrained G: {config_content['pretrained_G']} - {os.path.exists(config_content['pretrained_G'])}\")\nprint(f\"  Pretrained D: {config_content['pretrained_D']} - {os.path.exists(config_content['pretrained_D'])}\")\nprint(f\"  RMVPE: {os.path.join(weights_dir, 'rmvpe.pt')} - {os.path.exists(os.path.join(weights_dir, 'rmvpe.pt'))}\")\nprint(f\"  Hubert: {hubert_path} - {os.path.exists(hubert_path)}\")\nprint(f\"  GPUs: {config_content['gpus']}\")\n!head -n 20 {config_path}\n\n# Step 7: Install and Configure FileBrowser\nprint(\"\\n--- Step 7: Install FileBrowser ---\")\nos.chdir(\"/kaggle/working\")\nfilebrowser_dir = \"/kaggle/working/filebrowser_install\"\nfilebrowser_bin = os.path.join(filebrowser_dir, \"filebrowser\")\nfilebrowser_db = os.path.join(filebrowser_dir, \"filebrowser.db\")\nfilebrowser_config = os.path.join(filebrowser_dir, \".filebrowser.json\")\nfilebrowser_tar = \"linux-amd64-filebrowser.tar.gz\"\nfilebrowser_url = \"https://github.com/filebrowser/filebrowser/releases/download/v2.31.2/linux-amd64-filebrowser.tar.gz\"\nos.makedirs(filebrowser_dir, exist_ok=True)\n\nif os.path.exists(filebrowser_bin):\n    print(\"FileBrowser exists, skipping download\")\nelse:\n    print(f\"Downloading FileBrowser from {filebrowser_url}...\")\n    if os.path.exists(filebrowser_tar):\n        os.remove(filebrowser_tar)\n    try:\n        !wget -q {filebrowser_url} -O {filebrowser_tar}\n        !tar xvfz {filebrowser_tar} -C {filebrowser_dir}\n        if not os.path.exists(filebrowser_bin):\n            extracted_contents = os.listdir(filebrowser_dir)\n            if len(extracted_contents) == 1 and os.path.isdir(os.path.join(filebrowser_dir, extracted_contents[0])):\n                inner_dir = os.path.join(filebrowser_dir, extracted_contents[0])\n                if os.path.exists(os.path.join(inner_dir, \"filebrowser\")):\n                    for item in os.listdir(inner_dir):\n                        shutil.move(os.path.join(inner_dir, item), filebrowser_dir)\n                    shutil.rmtree(inner_dir)\n                else:\n                    raise FileNotFoundError(\"FileBrowser executable not found in subdirectory\")\n            else:\n                raise FileNotFoundError(\"FileBrowser executable not found\")\n        print(\"FileBrowser extracted\")\n        os.remove(filebrowser_tar)\n    except Exception as e:\n        print(f\"Error downloading/extracting FileBrowser: {e}\")\n        shutil.rmtree(filebrowser_dir, ignore_errors=True)\n        filebrowser_bin = None\n\nif filebrowser_bin and os.path.exists(filebrowser_bin):\n    print(\"Configuring FileBrowser...\")\n    try:\n        !chmod +x {filebrowser_bin}\n        db_flag = f\"--database={filebrowser_db}\"\n        config_flag = f\"--config={filebrowser_config}\"\n        if not os.path.exists(filebrowser_config):\n            !{filebrowser_bin} config init {config_flag} {db_flag}\n        !{filebrowser_bin} config set --root=\"/kaggle/working\" {config_flag} {db_flag}\n        !{filebrowser_bin} config set --address=\"0.0.0.0\" {config_flag} {db_flag}\n        !{filebrowser_bin} config set --port=\"8088\" {config_flag} {db_flag}\n        !{filebrowser_bin} config set --auth.method=\"noauth\" {config_flag} {db_flag}\n        !{filebrowser_bin} config set --branding.theme=\"dark\" {config_flag} {db_flag}\n        print(\"FileBrowser configured\")\n        !{filebrowser_bin} config cat {config_flag} {db_flag}\n    except Exception as e:\n        print(f\"Error configuring FileBrowser: {e}\")\n        filebrowser_bin = None\n\n# Step 8: Attempt Manual Training\nprint(\"\\n--- Step 8: Attempt Manual Training ---\")\nos.chdir(rvc_dir)\nprint(f\"Changed directory: {os.getcwd()}\")\n\nprint(\"Monitoring VRAM...\")\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    !nvidia-smi\n\ntraining_config_path = config_path\ntrain_script = os.path.join(\"infer\", \"modules\", \"train\", \"train.py\")\nif not os.path.exists(train_script):\n    print(f\"Error: Training script not found at {train_script}\")\nelif not os.path.exists(training_config_path):\n    print(f\"Error: Config not found at {training_config_path}\")\nelif not os.path.exists(config_content['data']['training_files']) or not os.path.getsize(config_content['data']['training_files']):\n    print(\"Error: Training metadata missing or empty\")\nelif not all(os.path.exists(path) for path in [config_content['pretrained_G'], config_content['pretrained_D']]):\n    print(\"Error: Pretrained models missing\")\nelse:\n    print(f\"Running: {python_executable} {train_script} --config {training_config_path}\")\n    try:\n        env = os.environ.copy()\n        env['CUDA_VISIBLE_DEVICES'] = config_content.get('gpus', '0')\n        cmd = [python_executable, train_script, \"--config\", training_config_path]\n        timeout_hours = 3\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)\n        print(\"--- Training Log ---\")\n        while True:\n            output = process.stdout.readline()\n            if output == '' and process.poll() is not None:\n                break\n            if output:\n                print(output.strip())\n        print(\"--- End Training Log ---\")\n        return_code = process.poll()\n        if return_code == 0:\n            print(\"\\n--- Training Completed ---\")\n            # Attempt index training\n            index_script = os.path.join(\"infer\", \"modules\", \"train\", \"index.py\")  # Hypothetical\n            if os.path.exists(index_script):\n                print(\"Attempting index training...\")\n                try:\n                    cmd = [python_executable, index_script, \"--model_name\", SPEAKER_NAME, \"--logs_dir\", os.path.join(rvc_dir, \"logs\", SPEAKER_NAME)]\n                    result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n                    print(\"Index training completed:\\n\", result.stdout)\n                except Exception as e:\n                    print(f\"Index training failed: {e}. Run manually in WebUI.\")\n        else:\n            print(f\"\\n--- Training Failed (Exit Code: {return_code}) ---\")\n            !nvidia-smi\n            !ldconfig -p | grep -E 'libcufft|libnvrtc|libcudnn|libcupti|libtorch'\n            !{python_executable} -c \"import torch; print(f'Torch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}'); print(f'CUDA Version: {torch.version.cuda}'); print(f'cuDNN: {torch.backends.cudnn.version()}'); print(f'Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \\'N/A\\'})\"\n            !{python_executable} -m pip list | grep -E 'torch|numpy|librosa|soundfile|scipy|numba|resampy|tqdm|einops|tensorboardX|fairseq|praat'\n    except subprocess.TimeoutExpired:\n        print(f\"\\nTraining timed out after {timeout_hours} hours\")\n        process.kill()\n    except Exception as e:\n        print(f\"Error running training: {e}\\n{traceback.format_exc()}\")\n\n# Step 9: Launch WebUI\nprint(\"\\n--- Step 9: Launch WebUI ---\")\nos.chdir(webui_dir)\nprint(f\"Changed directory: {os.getcwd()}\")\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nwebui_log_file = \"/kaggle/working/webui.log\"\ntunnel_log_file = \"/kaggle/working/tunnel.log\"\nopen(webui_log_file, 'w').close()\nopen(tunnel_log_file, 'w').close()\n\ndef run_webui(log_queue):\n    webui_ready = False\n    try:\n        print(\"Starting WebUI...\")\n        launch_script = \"launch.py\"\n        if not os.path.exists(launch_script):\n            for script in [\"app.py\", \"webui.py\", \"main.py\"]:\n                if os.path.exists(script):\n                    launch_script = script\n                    break\n            else:\n                log_queue.put(f\"ERROR: No WebUI launch script found in {os.getcwd()}\")\n                return\n        cmd = [python_executable, \"-u\", launch_script]\n        master_fd, slave_fd = pty.openpty()\n        process = subprocess.Popen(cmd, stdout=slave_fd, stderr=slave_fd, close_fds=True, text=True, bufsize=1, env=os.environ.copy())\n        os.close(slave_fd)\n        print(f\"WebUI started (PID: {process.pid})\")\n        log_queue.put(f\"WebUI started (PID: {process.pid})\")\n        buffer = \"\"\n        while process.poll() is None:\n            rlist, _, _ = select.select([master_fd], [], [], 0.1)\n            if rlist:\n                try:\n                    output = os.read(master_fd, 1024).decode('utf-8', errors='replace')\n                    if output:\n                        buffer += output\n                        while '\\n' in buffer:\n                            line, buffer = buffer.split('\\n', 1)\n                            print(f\"WebUI: {line.strip()}\")\n                            log_queue.put(line.strip())\n                            if \"Running on local URL:\" in line or \"Running on public URL:\" in line:\n                                if not webui_ready:\n                                    print(\"WebUI ready\")\n                                    log_queue.put(\"WEBUI_READY\")\n                                    webui_ready = True\n                            if \"Error\" in line or \"Traceback\" in line:\n                                print(f\"Potential error: {line.strip()}\")\n                    else:\n                        break\n                except OSError:\n                    break\n        exit_code = process.wait()\n        print(f\"WebUI terminated with exit code {exit_code}\")\n        log_queue.put(f\"WebUI terminated with exit code {exit_code}\")\n        try:\n            remaining_output = os.read(master_fd, 10240).decode('utf-8', errors='replace')\n            if remaining_output:\n                print(f\"WebUI (remaining): {remaining_output.strip()}\")\n                log_queue.put(remaining_output.strip())\n        except OSError:\n            pass\n        os.close(master_fd)\n    except Exception as e:\n        error_msg = f\"Error running WebUI: {e}\\n{traceback.format_exc()}\"\n        print(error_msg)\n        log_queue.put(error_msg)\n\ndef run_filebrowser(log_queue):\n    if not filebrowser_bin or not os.path.exists(filebrowser_bin):\n        log_queue.put(\"FileBrowser not available\")\n        return\n    try:\n        cmd = [filebrowser_bin, \"--config\", filebrowser_config, \"--database\", filebrowser_db]\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n        log_queue.put(f\"FileBrowser started (PID: {process.pid})\")\n        while process.poll() is None:\n            line = process.stdout.readline().strip()\n            if line:\n                log_queue.put(f\"FileBrowser: {line}\")\n            time.sleep(0.1)\n        log_queue.put(f\"FileBrowser terminated with exit code {process.poll()}\")\n    except Exception as e:\n        log_queue.put(f\"FileBrowser error: {e}\")\n\ndef start_ngrok_tunnel(port, log_queue):\n    if not NGROK_TOKEN:\n        log_queue.put(\"Ngrok token not available\")\n        return None\n    try:\n        from pyngrok import ngrok\n        print(f\"Starting Ngrok tunnel for port {port}...\")\n        public_url = ngrok.connect(port, proto=\"http\", bind_tls=True)\n        url_str = str(public_url)\n        print(f\"Ngrok tunnel: {url_str}\")\n        log_queue.put(f\"NGROK_URL:{url_str}\")\n        return public_url\n    except Exception as e:\n        log_queue.put(f\"Failed to start Ngrok: {e}\")\n        return None\n\ndef start_pinggy_tunnel(port, log_queue):\n    print(f\"Starting Pinggy tunnel for port {port}...\")\n    cmd = f\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p 80 -R0:localhost:{port} a.pinggy.io > {tunnel_log_file} 2>&1 &\"\n    try:\n        pinggy_process = subprocess.Popen(cmd, shell=True)\n        print(f\"Pinggy started (PID: {pinggy_process.pid})\")\n        log_queue.put(f\"Pinggy started (PID: {pinggy_process.pid})\")\n        found_url = None\n        max_attempts = 30\n        for attempt in range(max_attempts):\n            time.sleep(2)\n            try:\n                with open(tunnel_log_file, 'r') as file:\n                    log_content = file.read()\n                    import re\n                    match = re.search(r'(https?://\\w+\\.pinggy\\.link)', log_content)\n                    if match:\n                        found_url = match.group(1)\n                        print(f\"Pinggy URL: {found_url}\")\n                        log_queue.put(f\"PINGGY_URL:{found_url}\")\n                        break\n                    if \"Connection refused\" in log_content or \"failed\" in log_content.lower():\n                        log_queue.put(f\"Pinggy failed: {log_content[-200:]}\")\n                        return None\n            except Exception as e:\n                log_queue.put(f\"Error reading Pinggy log: {e}\")\n        if not found_url:\n            log_queue.put(f\"Pinggy URL not found after {max_attempts} attempts\")\n            try:\n                with open(tunnel_log_file, 'r') as f:\n                    print(f.read())\n            except:\n                pass\n            return None\n        return found_url\n    except Exception as e:\n        log_queue.put(f\"Failed to start Pinggy: {e}\")\n        return None\n\nwebui_port = 7860\nlog_queue = Queue()\np_webui = Process(target=run_webui, args=(log_queue,))\np_webui.start()\np_filebrowser = None\nif filebrowser_bin:\n    print(\"Starting FileBrowser...\")\n    p_filebrowser = Process(target=run_filebrowser, args=(log_queue,))\n    p_filebrowser.start()\n\ntunnel_url = None\ntunnel_type = None\nif NGROK_TOKEN:\n    print(\"Attempting Ngrok tunnel...\")\n    tunnel_url = start_ngrok_tunnel(webui_port, log_queue)\n    if tunnel_url:\n        tunnel_type = \"Ngrok\"\n    else:\n        print(\"Ngrok failed. Trying Pinggy...\")\n        tunnel_url = start_pinggy_tunnel(webui_port, log_queue)\n        if tunnel_url:\n            tunnel_type = \"Pinggy\"\nelse:\n    print(\"No Ngrok token. Trying Pinggy...\")\n    tunnel_url = start_pinggy_tunnel(webui_port, log_queue)\n    if tunnel_type:\n        tunnel_type = \"Pinggy\"\n\nif not tunnel_url:\n    print(\"WARNING: Failed to establish tunnel\")\nelse:\n    print(f\"\\n----------------------------------------------------\")\n    print(f\"✅ {tunnel_type or 'N/A'} tunnel started! ✅\")\n    tunnel_url_str = str(tunnel_url)\n    print(f\"   WebUI URL ({tunnel_type}): {tunnel_url_str}\")\n    if tunnel_type == \"Pinggy\":\n        https_url = tunnel_url_str.replace(\"http:\", \"https:\")\n        print(f\"   Try HTTPS: {https_url}\")\n    print(f\"----------------------------------------------------\")\n\nwebui_is_ready = False\nstart_time = time.time()\nmax_run_duration_hours = 8\nmax_run_duration_sec = max_run_duration_hours * 3600\nprint(f\"\\nMonitoring WebUI. Max duration: {max_run_duration_hours} hours\")\nif tunnel_url:\n    print(\"Access WebUI using the URL above\")\nelse:\n    print(\"WebUI may only be accessible locally\")\nprint(\"Training instructions below\")\ntry:\n    with open(webui_log_file, 'a', encoding='utf-8') as log_f:\n        log_f.write(f\"Monitoring started at {time.ctime()}\\n\")\n        while p_webui.is_alive():\n            if time.time() - start_time > max_run_duration_sec:\n                print(f\"\\nMax duration of {max_run_duration_hours} hours reached\")\n                log_f.write(f\"Timeout at {time.ctime()}\\n\")\n                p_webui.terminate()\n                if p_filebrowser and p_filebrowser.is_alive():\n                    p_filebrowser.terminate()\n                time.sleep(5)\n                break\n            while not log_queue.empty():\n                try:\n                    message = log_queue.get_nowait()\n                    log_f.write(f\"{time.strftime('%H:%M:%S')} | {message}\\n\")\n                    log_f.flush()\n                    if message == \"WEBUI_READY\" and not webui_is_ready:\n                        print(\">>> WebUI Ready <<<\")\n                        webui_is_ready = True\n                    elif \"Error:\" in message or \"Traceback (most recent call last):\" in message:\n                        print(f\"Log: {message}\")\n                    elif message.startswith(\"WebUI process terminated\"):\n                        print(f\"Log: {message}\")\n                except queue.Empty:\n                    break\n                except Exception as e:\n                    print(f\"Error processing log: {e}\")\n                    log_f.write(f\"Error processing log: {e}\\n\")\n            time.sleep(2)\nexcept KeyboardInterrupt:\n    print(\"\\nKeyboardInterrupt received. Shutting down...\")\n    try:\n        if log_f and not log_f.closed:\n            log_f.write(f\"KeyboardInterrupt at {time.ctime()}\\n\")\n    except NameError:\n        pass\n    if p_webui.is_alive():\n        p_webui.terminate()\n    if p_filebrowser and p_filebrowser.is_alive():\n        p_filebrowser.terminate()\nexcept Exception as e:\n    print(f\"Error in monitoring: {e}\\n{traceback.format_exc()}\")\n    try:\n        if log_f and not log_f.closed:\n            log_f.write(f\"Error in monitoring: {e}\\n{traceback.format_exc()}\\n\")\n    except NameError:\n        pass\n    if p_webui.is_alive():\n        p_webui.terminate()\n    if p_filebrowser and p_filebrowser.is_alive():\n        p_filebrowser.terminate()\n\nif p_webui.is_alive():\n    p_webui.join(timeout=30)\n    if p_webui.is_alive():\n        print(\"WebUI did not terminate gracefully. Killing...\")\n        p_webui.kill()\nif p_filebrowser and p_filebrowser.is_alive():\n    p_filebrowser.join(timeout=30)\n    if p_filebrowser.is_alive():\n        print(\"FileBrowser did not terminate gracefully. Killing...\")\n        p_filebrowser.kill()\n\nprint(\"WebUI monitoring finished\")\nprint(f\"Logs: {webui_log_file}, {tunnel_log_file}\")\n\n# Training instructions\nfb_url = \"Unavailable (Tunnel Error?)\"\nif tunnel_url and filebrowser_bin:\n    base_tunnel_url = str(tunnel_url).replace(\"https://\", \"\").replace(\"http://\", \"\").split(':')[0]\n    fb_url = f\"http://{base_tunnel_url}:8088\"\nfilebrowser_info = f\"2.  **FileBrowser:** Access at: {fb_url}\" if filebrowser_bin else \"2.  **FileBrowser:** Not installed\"\n\n# Debug config_content state\nprint(\"Debug: config_content keys:\", list(config_content.keys()))\nprint(f\"Debug: GPUs in config: {config_content.get('gpus', 'Not set')}\")\n\n# Use fallback for GPUs\ngpus_value = config_content.get('gpus', '0')  # Fallback to '0' if missing\n\nprint(f\"\"\"\n=========================================================\n          RVC WebUI Training Instructions\n=========================================================\n\n1.  **Access WebUI:** Open the {tunnel_type or 'N/A'} URL: {tunnel_url_str if tunnel_url else 'Local access only'}\n    * If the URL fails, check {tunnel_log_file}\n    * For Pinggy, try HTTPS: {https_url if tunnel_type == \"Pinggy\" else 'N/A'}\n\n2.  **Navigate to Training:** Find the 'Training' or 'Train' tab\n\n3.  **Configure Training:**\n    * **Experiment Name:** '{SPEAKER_NAME}'\n    * **Dataset Path:** `{output_wav_dir}` or `{metadata_train_dst}'\n    * **Sample Rate:** `{TARGET_SR}'\n    * **Pitch Extraction:** 'rmvpe'\n    * **GPUs:** '{gpus_value}'\n    * **Epochs:** `{config_content['train']['epochs']}'\n    * **Batch Size:** `{config_content['train']['batch_size']}'\n    * **Save Frequency:** `{config_content['save_every_epoch']}' epochs\n    * **Pretrained Models:**\n        * G: `{config_content['pretrained_G']}'\n        * D: `{config_content['pretrained_D']}'\n\n4.  **Start Training:** Click 'Start Training' or 'Train Model'\n\n5.  **Monitor:** Watch progress in WebUI console. Check VRAM in Kaggle's Accelerator panel. Reduce batch size if OOM occurs\n\n6.  **After Training:**\n    * Generate index file in 'Index Training' or 'Inference' tab\n    * Select '{SPEAKER_NAME}' model and click 'Train Index'\n\n7.  **Test Inference:** Use 'Inference' tab, select '{SPEAKER_NAME}' model, ensure index loaded, upload audio, click 'Convert'\n\n8.  **Download Outputs:** See Step 10\n\n=========================================================\n\"\"\")\n\n# Step 10: Save Outputs\nprint(\"\\n--- Step 10: Download Outputs ---\")\nmodel_output_dir_rvc = os.path.join(rvc_dir, \"weights\")\nmodel_output_dir_webui = os.path.join(webui_dir, \"models\", \"checkpoints\", SPEAKER_NAME)\nindex_output_dir_rvc_glob = os.path.join(rvc_dir, \"logs\", SPEAKER_NAME, \"added_*.index\")\nindex_output_dir_webui_glob = os.path.join(webui_dir, \"models\", \"checkpoints\", SPEAKER_NAME, \"add_*.index\")\ninference_output_dir_webui = os.path.join(webui_dir, \"outputs\")\nlogs_dir_rvc = os.path.join(rvc_dir, \"logs\", SPEAKER_NAME)\n\nprint(f\"\"\"\n=========================================================\n                Training Complete & Outputs\n=========================================================\n\nDownload files using:\n\n1.  **Kaggle Output:** Check \"Data\" -> \"Output\" in Kaggle UI\n{filebrowser_info}\n3.  **WebUI Download:** Use WebUI download buttons if available\n\n**File Locations:**\n\n* **Model (.pth):**\n    * RVC: `{model_output_dir_rvc}/{SPEAKER_NAME}_<epoch>.pth`\n    * WebUI: `{model_output_dir_webui}/<name>.pth`\n* **Index (.index):**\n    * RVC: `{index_output_dir_rvc_glob}`\n    * WebUI: `{index_output_dir_webui_glob}`\n* **Inference Outputs (.wav):**\n    * WebUI: `{inference_output_dir_webui}/`\n* **Logs:**\n    * RVC: `{logs_dir_rvc}/`\n    * WebUI: `{webui_log_file}`\n    * Tunnel: `{tunnel_log_file}`\n\n**Using Model Locally:**\n\n1. Install RVC locally (see RVC GitHub)\n2. Download `{SPEAKER_NAME}.pth` (latest/best epoch)\n3. Download `.index` file (`added_...` or `add_...`)\n4. Place `.pth` in RVC `weights`\n5. Place `.index` in RVC `logs/{SPEAKER_NAME}/`\n6. Use RVC WebUI/CLI for inference\n\n=========================================================\n\"\"\")\n\nprint(\"Final output check (last 5 files):\")\n!ls -lhtr {model_output_dir_rvc} | tail -n 5 || echo \"Not found: {model_output_dir_rvc}\"\n!ls -lhtr {model_output_dir_webui} | tail -n 5 || echo \"Not found: {model_output_dir_webui}\"\n!ls -lhtr {index_output_dir_rvc_glob} 2>/dev/null | tail -n 5 || echo \"No index files: {index_output_dir_rvc_glob}\"\n!ls -lhtr {index_output_dir_webui_glob} 2>/dev/null | tail -n 5 || echo \"No index files: {index_output_dir_webui_glob}\"\n!ls -lhtr {inference_output_dir_webui} | tail -n 5 || echo \"Not found: {inference_output_dir_webui}\"\n!ls -lhtr {logs_dir_rvc} | tail -n 5 || echo \"Not found: {logs_dir_rvc}\"\n\nprint(\"\\nNotebook execution finished\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-04-30T14:04:47.931193Z","iopub.execute_input":"2025-04-30T14:04:47.931462Z","iopub.status.idle":"2025-04-30T14:10:16.658033Z","shell.execute_reply.started":"2025-04-30T14:04:47.931442Z","shell.execute_reply":"2025-04-30T14:10:16.657052Z"}},"outputs":[{"name":"stdout","text":"============================================================\nIMPORTANT KAGGLE SETTINGS:\n- Ensure 'Accelerator' is set to GPU (P100 or T4 x2 recommended).\n- Ensure 'Internet' is turned ON in the Settings panel.\n============================================================\nChecking disk space...\nAvailable space: 20.00 GB\nInstalling cuDNN for CUDA 11.8...\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  libcudnn8 libcudnn8-dev\n0 upgraded, 2 newly installed, 0 to remove and 153 not upgraded.\nNeed to get 878 MB of archives.\nAfter this operation, 2,366 MB of additional disk space will be used.\nGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8 8.9.7.29-1+cuda11.8 [441 MB]\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8-dev 8.9.7.29-1+cuda11.8 [437 MB]\nFetched 878 MB in 10s (86.4 MB/s)                                                                   \nSelecting previously unselected package libcudnn8.\n(Reading database ... 128691 files and directories currently installed.)\nPreparing to unpack .../libcudnn8_8.9.7.29-1+cuda11.8_amd64.deb ...\nUnpacking libcudnn8 (8.9.7.29-1+cuda11.8) ...\nSelecting previously unselected package libcudnn8-dev.\nPreparing to unpack .../libcudnn8-dev_8.9.7.29-1+cuda11.8_amd64.deb ...\nUnpacking libcudnn8-dev (8.9.7.29-1+cuda11.8) ...\nSetting up libcudnn8 (8.9.7.29-1+cuda11.8) ...\nSetting up libcudnn8-dev (8.9.7.29-1+cuda11.8) ...\nupdate-alternatives: warning: forcing reinstallation of alternative /usr/include/x86_64-linux-gnu/cudnn_v9.h because link group libcudnn is broken\nupdate-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v8.h to provide /usr/include/cudnn.h (libcudnn) in manual mode\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\nChecking for residual libcupti files...\nInstalling libcupti for CUDA 11.8...\nCUPTI .deb install failed: libcupti.so.11.8 not found at /kaggle/working/cupti/cuda_install/usr/local/cuda-11.8/extras/CUPTI/lib64/libcupti.so.11.8. Attempting CUDA Toolkit...\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\nlrwxrwxrwx 1 root root      42 Apr 30 14:09 /usr/lib/x86_64-linux-gnu/libcupti.so -> /usr/lib/x86_64-linux-gnu/libcupti.so.11.8\n-rwxr-xr-x 1 root root 7199856 Apr 30 14:09 /usr/lib/x86_64-linux-gnu/libcupti.so.11.8\nLD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n\n--- Verifying Installations ---\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\nWed Apr 30 14:09:42 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n148164028    140 -rw-r--r--   1 root     root       142008 Nov 30  2023 /usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\n148164042      0 lrwxrwxrwx   1 root     root           17 Nov 30  2023 /usr/lib/x86_64-linux-gnu/libcudnn.so.8 -> libcudnn.so.8.9.7\n148164024   7032 -rwxr-xr-x   1 root     root      7199856 Apr 30 14:09 /usr/lib/x86_64-linux-gnu/libcupti.so.11.8\n\tlibcupti.so.12 (libc6,x86-64) => /usr/local/cuda-12/targets/x86_64-linux/lib/libcupti.so.12\n\tlibcupti.so.11.8 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcupti.so.11.8\n\tlibcupti.so (libc6,x86-64) => /usr/local/cuda-12/targets/x86_64-linux/lib/libcupti.so\n\tlibcupti.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libcupti.so\nVersion symbols section '.gnu.version' contains 416 entries:\nVersion definition section '.gnu.version_d' contains 2 entries:\nVersion needs section '.gnu.version_r' contains 7 entries:\n  000000: Version: 1  File: ld-linux-x86-64.so.2  Cnt: 1\n  0x0010:   Name: GLIBC_2.3  Flags: none  Version: 14\n  0x0020: Version: 1  File: libm.so.6  Cnt: 1\n  0x0030:   Name: GLIBC_2.2.5  Flags: none  Version: 12\n  0x0040: Version: 1  File: libdl.so.2  Cnt: 1\n  0x0050:   Name: GLIBC_2.2.5  Flags: none  Version: 10\n  0x0060: Version: 1  File: librt.so.1  Cnt: 1\n  0x0070:   Name: GLIBC_2.2.5  Flags: none  Version: 9\n  0x0080: Version: 1  File: libgcc_s.so.1  Cnt: 3\n  0x0090:   Name: GCC_3.3  Flags: none  Version: 15\n  0x00a0:   Name: GCC_3.0  Flags: none  Version: 8\n  0x00b0:   Name: GCC_4.2.0  Flags: none  Version: 7\n  0x00c0: Version: 1  File: libc.so.6  Cnt: 4\n  0x00d0:   Name: GLIBC_2.3.3  Flags: none  Version: 13\n  0x00e0:   Name: GLIBC_2.6  Flags: none  Version: 11\n  0x00f0:   Name: GLIBC_2.3  Flags: none  Version: 6\n  0x0100:   Name: GLIBC_2.2.5  Flags: none  Version: 4\n  0x0110: Version: 1  File: libpthread.so.0  Cnt: 2\n  0x0120:   Name: GLIBC_2.3.2  Flags: none  Version: 5\n  0x0130:   Name: GLIBC_2.2.5  Flags: none  Version: 3\n\n--- Testing torch import and CUDA ---\nTorch version: 2.5.1+cu124\nTorch CUDA available: True\nCUDA version: 12.4\ncuDNN version: 90300\nGPUs: 2\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\n\n--- GPU Configuration ---\nCUDA available: True\nGPU count: 2\nGPU 0: Tesla T4, VRAM: 14.74 GB\nGPU 1: Tesla T4, VRAM: 14.74 GB\nDetected: T4 x2\nWed Apr 30 14:09:45 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nInstalling dependencies with Mamba...\n/usr/local/lib/python3.11/dist-packages/clint/textui/prompt.py:68: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if prompt[-1] is not ' ':\nusage: mamba [-h] [--version] [--slow SLOW] [--enable-coverage] [--coverage-file COVERAGE_FILE]\n             [--format FORMAT] [--no-color] [--tags TAGS]\n             [specs ...]\nmamba: error: unrecognized arguments: -n base --prune\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nbuild-essential is already the newest version (12.9ubuntu3).\ng++ is already the newest version (4:11.2.0-1ubuntu1).\ng++ set to manually installed.\nlibvorbis-dev is already the newest version (1.3.7-1build2).\nlibvorbis-dev set to manually installed.\nlibopenblas-dev is already the newest version (0.3.20+ds-1).\ncmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\ngit is already the newest version (1:2.34.1-1ubuntu1.12).\nlibsndfile1-dev is already the newest version (1.0.31-2ubuntu0.2).\nunzip is already the newest version (6.0-26ubuntu3.2).\nThe following additional packages will be installed:\n  libaria2-0 libc-ares2 libomp-14-dev libomp5-14 libout123-0 libsyn123-0 swig4.0\nSuggested packages:\n  libomp-14-doc swig-doc swig-examples swig4.0-examples swig4.0-doc\nThe following NEW packages will be installed:\n  aria2 libaria2-0 libc-ares2 libmpg123-dev libomp-14-dev libomp-dev libomp5-14 libout123-0\n  libsyn123-0 swig swig4.0\n0 upgraded, 11 newly installed, 0 to remove and 155 not upgraded.\nNeed to get 3,549 kB of archives.\nAfter this operation, 20.6 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.3 [45.1 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libout123-0 amd64 1.29.3-1ubuntu0.1 [31.4 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsyn123-0 amd64 1.29.3-1ubuntu0.1 [97.2 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmpg123-dev amd64 1.29.3-1ubuntu0.1 [53.6 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libomp5-14 amd64 1:14.0.0-1ubuntu1.1 [389 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libomp-14-dev amd64 1:14.0.0-1ubuntu1.1 [347 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libomp-dev amd64 1:14.0-55~exp2 [3,074 B]\nFetched 3,549 kB in 0s (25.7 MB/s)      \nSelecting previously unselected package libc-ares2:amd64.\n(Reading database ... 128741 files and directories currently installed.)\nPreparing to unpack .../00-libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\nUnpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\nSelecting previously unselected package libaria2-0:amd64.\nPreparing to unpack .../01-libaria2-0_1.36.0-1_amd64.deb ...\nUnpacking libaria2-0:amd64 (1.36.0-1) ...\nSelecting previously unselected package aria2.\nPreparing to unpack .../02-aria2_1.36.0-1_amd64.deb ...\nUnpacking aria2 (1.36.0-1) ...\nSelecting previously unselected package libout123-0:amd64.\nPreparing to unpack .../03-libout123-0_1.29.3-1ubuntu0.1_amd64.deb ...\nUnpacking libout123-0:amd64 (1.29.3-1ubuntu0.1) ...\nSelecting previously unselected package libsyn123-0:amd64.\nPreparing to unpack .../04-libsyn123-0_1.29.3-1ubuntu0.1_amd64.deb ...\nUnpacking libsyn123-0:amd64 (1.29.3-1ubuntu0.1) ...\nSelecting previously unselected package libmpg123-dev:amd64.\nPreparing to unpack .../05-libmpg123-dev_1.29.3-1ubuntu0.1_amd64.deb ...\nUnpacking libmpg123-dev:amd64 (1.29.3-1ubuntu0.1) ...\nSelecting previously unselected package libomp5-14:amd64.\nPreparing to unpack .../06-libomp5-14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\nUnpacking libomp5-14:amd64 (1:14.0.0-1ubuntu1.1) ...\nSelecting previously unselected package libomp-14-dev.\nPreparing to unpack .../07-libomp-14-dev_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\nUnpacking libomp-14-dev (1:14.0.0-1ubuntu1.1) ...\nSelecting previously unselected package swig4.0.\nPreparing to unpack .../08-swig4.0_4.0.2-1ubuntu1_amd64.deb ...\nUnpacking swig4.0 (4.0.2-1ubuntu1) ...\nSelecting previously unselected package swig.\nPreparing to unpack .../09-swig_4.0.2-1ubuntu1_all.deb ...\nUnpacking swig (4.0.2-1ubuntu1) ...\nSelecting previously unselected package libomp-dev:amd64.\nPreparing to unpack .../10-libomp-dev_1%3a14.0-55~exp2_amd64.deb ...\nUnpacking libomp-dev:amd64 (1:14.0-55~exp2) ...\nSetting up libomp5-14:amd64 (1:14.0.0-1ubuntu1.1) ...\nSetting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\nSetting up libout123-0:amd64 (1.29.3-1ubuntu0.1) ...\nSetting up libsyn123-0:amd64 (1.29.3-1ubuntu0.1) ...\nSetting up swig4.0 (4.0.2-1ubuntu1) ...\nSetting up libomp-14-dev (1:14.0.0-1ubuntu1.1) ...\nSetting up libomp-dev:amd64 (1:14.0-55~exp2) ...\nSetting up libaria2-0:amd64 (1.36.0-1) ...\nSetting up swig (4.0.2-1ubuntu1) ...\nSetting up aria2 (1.36.0-1) ...\nSetting up libmpg123-dev:amd64 (1.29.3-1ubuntu0.1) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n\n--- Verifying packages ---\n/bin/bash: line 1: /opt/conda/bin/python: No such file or directory\n/bin/bash: line 1: /opt/conda/bin/python: No such file or directory\n/bin/bash: line 1: {python_executable}: command not found\n/bin/bash: line 1: {python_executable}: command not found\n/bin/bash: line 1: {python_executable}: command not found\n/bin/bash: line 1: {python_executable}: command not found\n/bin/bash: line 1: {python_executable}: command not found\naria2 version 1.36.0\nffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n\n--- Step 2: Install VGMStream ---\nDownloading VGMStream (attempt 1/3)...\nDownloaded VGMStream ZIP\nUnzipping /kaggle/working/tools/vgmstream-linux-cli.zip...\nArchive:  /kaggle/working/tools/vgmstream-linux-cli.zip\n  inflating: /kaggle/working/tools/vgmstream_cli_extracted/vgmstream-cli  \nFound vgmstream-cli at: /kaggle/working/tools/vgmstream_cli_extracted/vgmstream-cli\nDeleted /kaggle/working/tools/vgmstream-linux-cli.zip\nVGMStream verification failed: Command '['/kaggle/working/tools/vgmstream_cli_extracted/vgmstream-cli', '--version']' returned non-zero exit status 1.\nTrying ww2ogg as fallback...\nRequirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.1-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-25.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.1\n\u001b[31mERROR: Could not find a version that satisfies the requirement ww2ogg==0.9.0 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for ww2ogg==0.9.0\u001b[0m\u001b[31m\n\u001b[0mFailed to install ww2ogg: No module named 'ww2ogg'\n\n--- Step 3: Install Ngrok ---\nCollecting pyngrok==7.2.0\n  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok==7.2.0) (6.0.2)\nDownloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.2.0\npyngrok installed\nCould not retrieve Ngrok token: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 81586794 and label NGROK_AUTH_TOKEN.'], 'error': {'code': 5, 'details': []}, 'wasSuccessful': False}.\nSet NGROK_TOKEN manually or add it as Kaggle Secret 'NGROK_AUTH_TOKEN'\nNgrok token not set. Falling back to Pinggy.\n\n--- Step 4: Clone RVC Repositories ---\nCurrent directory: /kaggle/working\nTesting GitHub connectivity (https://github.com)...\nGitHub connectivity verified\nTesting Hugging Face connectivity (https://huggingface.co)...\nHugging Face connectivity verified\nCloning https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI.git into /kaggle/working/RVC...\nClone attempt 1/3...\nCloned RVC\nCloning https://github.com/ddPn08/rvc-webui.git into /kaggle/working/rvc-webui...\nClone attempt 1/3...\nCloned rvc-webui\n\n--- Downloading Pretrained Models ---\nChecking disk space for f0G40k.pth (~150MB)...\nDownloading f0G40k.pth with aria2c (attempt 1/2)...\naria2c successful for f0G40k.pth\nChecking disk space for f0D40k.pth (~150MB)...\nDownloading f0D40k.pth with aria2c (attempt 1/2)...\naria2c successful for f0D40k.pth\nChecking disk space for HP5-主旋律人声vocals+其他instrumentals.pth (~150MB)...\nDownloading HP5-主旋律人声vocals+其他instrumentals.pth with aria2c (attempt 1/2)...\naria2c successful for HP5-主旋律人声vocals+其他instrumentals.pth\nChecking disk space for hubert_base.pt (~350MB)...\nDownloading hubert_base.pt with aria2c (attempt 1/2)...\naria2c successful for hubert_base.pt\nChecking disk space for rmvpe.pt (~5MB)...\nDownloading rmvpe.pt with aria2c (attempt 1/2)...\naria2c successful for rmvpe.pt\nAll pretrained models downloaded or present\n\nVerifying models:\n-rw-r--r-- 1 root root 137M Apr 30 14:10 /kaggle/working/RVC/pretrained_v2/f0D40k.pth\n-rw-r--r-- 1 root root  70M Apr 30 14:10 /kaggle/working/RVC/pretrained_v2/f0G40k.pth\n-rw-r--r-- 1 root root 61M Apr 30 14:10 /kaggle/working/RVC/uvr5_weights/HP5-主旋律人声vocals+其他instrumentals.pth\n-rw-r--r-- 1 root root 181M Apr 30 14:10 /kaggle/working/RVC/hubert_base.pt\n-rw-r--r-- 1 root root 173M Apr 30 14:10 /kaggle/working/RVC/weights/rmvpe.pt\n\n--- Step 5: Set Up Dataset ---\nNo complete cached dataset. Processing...\n\n--- Starting Dataset Processing ---\nWarning: No WEM conversion tool. Assuming WAV input\nPreprocessing 556 WAV files with 4 processes...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2284133603.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning: No WEM conversion tool. Assuming WAV input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0mtemp_wav_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_wem_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m     \u001b[0mpreprocessing_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_audio_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_wav_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_wav_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_SR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0mrename_potential_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_wav_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mmetadata_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"metadata.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2284133603.py\u001b[0m in \u001b[0;36mpreprocess_audio_parallel\u001b[0;34m(input_dir, output_dir, target_sr)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0msuccessful_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_audio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles_to_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         '''\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1}]}